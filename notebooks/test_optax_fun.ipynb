{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-05T18:49:11.333828Z",
     "start_time": "2024-11-05T18:49:11.330074Z"
    }
   },
   "source": [
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T18:49:11.357028Z",
     "start_time": "2024-11-05T18:49:11.345236Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import numpy as np\n",
    "\n",
    "BATCH_SIZE = 5\n",
    "NUM_TRAIN_STEPS = 1_000\n",
    "RAW_TRAINING_DATA = np.random.randint(255, size=(NUM_TRAIN_STEPS, BATCH_SIZE, 1))\n",
    "\n",
    "TRAINING_DATA = np.unpackbits(RAW_TRAINING_DATA.astype(np.uint8), axis=-1)\n",
    "LABELS = jax.nn.one_hot(RAW_TRAINING_DATA % 2, 2).astype(jnp.float32).reshape(NUM_TRAIN_STEPS, BATCH_SIZE, 2)\n",
    "\n",
    "initial_params = {\n",
    "    'hidden': jax.random.normal(shape=[8, 32], key=jax.random.PRNGKey(0)),\n",
    "    'output': jax.random.normal(shape=[32, 2], key=jax.random.PRNGKey(1)),\n",
    "}\n",
    "\n",
    "\n",
    "def net(x: jnp.ndarray, params: optax.Params) -> jnp.ndarray:\n",
    "  x = jnp.dot(x, params['hidden'])\n",
    "  x = jax.nn.relu(x)\n",
    "  x = jnp.dot(x, params['output'])\n",
    "  return x\n",
    "\n",
    "\n",
    "def loss(params: optax.Params, batch: jnp.ndarray, labels: jnp.ndarray) -> jnp.ndarray:\n",
    "  y_hat = net(batch, params)\n",
    "\n",
    "  # optax also provides a number of common loss functions.\n",
    "  loss_value = optax.sigmoid_binary_cross_entropy(y_hat, labels).sum(axis=-1)\n",
    "\n",
    "  return loss_value.mean()\n",
    "\n",
    "def fit(params: optax.Params, optimizer: optax.GradientTransformation) -> optax.Params:\n",
    "  opt_state = optimizer.init(params)\n",
    "\n",
    "  @jax.jit\n",
    "  def step(params, opt_state, batch, labels):\n",
    "    loss_value, grads = jax.value_and_grad(loss)(params, batch, labels)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params, loss_value, batch)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss_value\n",
    "\n",
    "  for i, (batch, labels) in enumerate(zip(TRAINING_DATA, LABELS)):\n",
    "    params, opt_state, loss_value = step(params, opt_state, batch, labels)\n",
    "    if i % 100 == 0:\n",
    "      print(f'step {i}, loss: {loss_value}')\n",
    "\n",
    "  return params"
   ],
   "id": "444f923a7bfae794",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T18:49:12.517579Z",
     "start_time": "2024-11-05T18:49:11.359024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from lapnet.spring_update.SPRING_optax import scale_by_spring\n",
    "\n",
    "optimizer = scale_by_spring(log_psi_apply=net)\n",
    "params = fit(initial_params, optimizer)"
   ],
   "id": "d4975103f698de2f",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Using a non-tuple sequence for multidimensional indexing is not allowed; use `arr[tuple(seq)]` instead of `arr[seq]`. See https://github.com/google/jax/issues/4564 for more information.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlapnet\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mSPRING_optax\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m scale_by_spring\n\u001B[1;32m      3\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m scale_by_spring(log_psi_apply\u001B[38;5;241m=\u001B[39mnet)\n\u001B[0;32m----> 4\u001B[0m params \u001B[38;5;241m=\u001B[39m \u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43minitial_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[5], line 45\u001B[0m, in \u001B[0;36mfit\u001B[0;34m(params, optimizer)\u001B[0m\n\u001B[1;32m     42\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m params, opt_state, loss_value\n\u001B[1;32m     44\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, (batch, labels) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mzip\u001B[39m(TRAINING_DATA, LABELS)):\n\u001B[0;32m---> 45\u001B[0m   params, opt_state, loss_value \u001B[38;5;241m=\u001B[39m \u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopt_state\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     46\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m100\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m     47\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstep \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss_value\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "    \u001B[0;31m[... skipping hidden 11 frame]\u001B[0m\n",
      "Cell \u001B[0;32mIn[5], line 40\u001B[0m, in \u001B[0;36mfit.<locals>.step\u001B[0;34m(params, opt_state, batch, labels)\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;129m@jax\u001B[39m\u001B[38;5;241m.\u001B[39mjit\n\u001B[1;32m     38\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(params, opt_state, batch, labels):\n\u001B[1;32m     39\u001B[0m   loss_value, grads \u001B[38;5;241m=\u001B[39m jax\u001B[38;5;241m.\u001B[39mvalue_and_grad(loss)(params, batch, labels)\n\u001B[0;32m---> 40\u001B[0m   updates, opt_state \u001B[38;5;241m=\u001B[39m \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopt_state\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_value\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     41\u001B[0m   params \u001B[38;5;241m=\u001B[39m optax\u001B[38;5;241m.\u001B[39mapply_updates(params, updates)\n\u001B[1;32m     42\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m params, opt_state, loss_value\n",
      "File \u001B[0;32m~/Desktop/LapNet/lapnet/SPRING_optax.py:129\u001B[0m, in \u001B[0;36mscale_by_spring.<locals>.update_fn\u001B[0;34m(updates, state, params, centered_energies, data)\u001B[0m\n\u001B[1;32m    126\u001B[0m nchains \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    127\u001B[0m prev_grad \u001B[38;5;241m=\u001B[39m state\u001B[38;5;241m.\u001B[39mgradient\n\u001B[0;32m--> 129\u001B[0m log_psi_grads \u001B[38;5;241m=\u001B[39m \u001B[43mbatch_raveled_log_psi_grad\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m/\u001B[39m jnp\u001B[38;5;241m.\u001B[39msqrt(\n\u001B[1;32m    130\u001B[0m     nchains\n\u001B[1;32m    131\u001B[0m )\n\u001B[1;32m    133\u001B[0m \u001B[38;5;66;03m# I think this all has to be written using tree syntax\u001B[39;00m\n\u001B[1;32m    134\u001B[0m prev_grad, unravel_fn \u001B[38;5;241m=\u001B[39m jax\u001B[38;5;241m.\u001B[39mflatten_util\u001B[38;5;241m.\u001B[39mravel_pytree(prev_grad)\n",
      "    \u001B[0;31m[... skipping hidden 3 frame]\u001B[0m\n",
      "File \u001B[0;32m~/Desktop/LapNet/lapnet/SPRING_optax.py:113\u001B[0m, in \u001B[0;36mscale_by_spring.<locals>.raveled_log_psi_grad\u001B[0;34m(params, positions)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mraveled_log_psi_grad\u001B[39m(params: P, positions: Array) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Array:\n\u001B[0;32m--> 113\u001B[0m     log_grads \u001B[38;5;241m=\u001B[39m \u001B[43mjax\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgrad\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlog_psi_apply\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpositions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m jax\u001B[38;5;241m.\u001B[39mflatten_util\u001B[38;5;241m.\u001B[39mravel_pytree(log_grads)[\u001B[38;5;241m0\u001B[39m]\n",
      "    \u001B[0;31m[... skipping hidden 10 frame]\u001B[0m\n",
      "Cell \u001B[0;32mIn[5], line 20\u001B[0m, in \u001B[0;36mnet\u001B[0;34m(x, params)\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mnet\u001B[39m(x: jnp\u001B[38;5;241m.\u001B[39mndarray, params: optax\u001B[38;5;241m.\u001B[39mParams) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m jnp\u001B[38;5;241m.\u001B[39mndarray:\n\u001B[0;32m---> 20\u001B[0m   x \u001B[38;5;241m=\u001B[39m jnp\u001B[38;5;241m.\u001B[39mdot(x, \u001B[43mparams\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mhidden\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m)\n\u001B[1;32m     21\u001B[0m   x \u001B[38;5;241m=\u001B[39m jax\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mrelu(x)\n\u001B[1;32m     22\u001B[0m   x \u001B[38;5;241m=\u001B[39m jnp\u001B[38;5;241m.\u001B[39mdot(x, params[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124moutput\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "    \u001B[0;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "File \u001B[0;32m~/anaconda3/envs/qmc/lib/python3.9/site-packages/jax/_src/numpy/lax_numpy.py:3815\u001B[0m, in \u001B[0;36m_rewriting_take\u001B[0;34m(arr, idx, indices_are_sorted, unique_indices, mode, fill_value)\u001B[0m\n\u001B[1;32m   3809\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28misinstance\u001B[39m(aval, core\u001B[38;5;241m.\u001B[39mDShapedArray) \u001B[38;5;129;01mand\u001B[39;00m aval\u001B[38;5;241m.\u001B[39mshape \u001B[38;5;241m==\u001B[39m () \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[1;32m   3810\u001B[0m         dtypes\u001B[38;5;241m.\u001B[39missubdtype(aval\u001B[38;5;241m.\u001B[39mdtype, np\u001B[38;5;241m.\u001B[39minteger) \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[1;32m   3811\u001B[0m         \u001B[38;5;129;01mnot\u001B[39;00m dtypes\u001B[38;5;241m.\u001B[39missubdtype(aval\u001B[38;5;241m.\u001B[39mdtype, dtypes\u001B[38;5;241m.\u001B[39mbool_) \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[1;32m   3812\u001B[0m         \u001B[38;5;28misinstance\u001B[39m(arr\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;28mint\u001B[39m)):\n\u001B[1;32m   3813\u001B[0m       \u001B[38;5;28;01mreturn\u001B[39;00m lax\u001B[38;5;241m.\u001B[39mdynamic_index_in_dim(arr, idx, keepdims\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m-> 3815\u001B[0m treedef, static_idx, dynamic_idx \u001B[38;5;241m=\u001B[39m \u001B[43m_split_index_for_jit\u001B[49m\u001B[43m(\u001B[49m\u001B[43midx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3816\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _gather(arr, treedef, static_idx, dynamic_idx, indices_are_sorted,\n\u001B[1;32m   3817\u001B[0m                unique_indices, mode, fill_value)\n",
      "File \u001B[0;32m~/anaconda3/envs/qmc/lib/python3.9/site-packages/jax/_src/numpy/lax_numpy.py:3890\u001B[0m, in \u001B[0;36m_split_index_for_jit\u001B[0;34m(idx, shape)\u001B[0m\n\u001B[1;32m   3885\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Splits indices into necessarily-static and dynamic parts.\u001B[39;00m\n\u001B[1;32m   3886\u001B[0m \n\u001B[1;32m   3887\u001B[0m \u001B[38;5;124;03mUsed to pass indices into `jit`-ted function.\u001B[39;00m\n\u001B[1;32m   3888\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   3889\u001B[0m \u001B[38;5;66;03m# Convert list indices to tuples in cases (deprecated by NumPy.)\u001B[39;00m\n\u001B[0;32m-> 3890\u001B[0m idx \u001B[38;5;241m=\u001B[39m \u001B[43m_eliminate_deprecated_list_indexing\u001B[49m\u001B[43m(\u001B[49m\u001B[43midx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3892\u001B[0m \u001B[38;5;66;03m# Expand any (concrete) boolean indices. We can then use advanced integer\u001B[39;00m\n\u001B[1;32m   3893\u001B[0m \u001B[38;5;66;03m# indexing logic to handle them.\u001B[39;00m\n\u001B[1;32m   3894\u001B[0m idx \u001B[38;5;241m=\u001B[39m _expand_bool_indices(idx, shape)\n",
      "File \u001B[0;32m~/anaconda3/envs/qmc/lib/python3.9/site-packages/jax/_src/numpy/lax_numpy.py:4170\u001B[0m, in \u001B[0;36m_eliminate_deprecated_list_indexing\u001B[0;34m(idx)\u001B[0m\n\u001B[1;32m   4166\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   4167\u001B[0m     msg \u001B[38;5;241m=\u001B[39m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing a non-tuple sequence for multidimensional indexing is not allowed; \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   4168\u001B[0m            \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muse `arr[array(seq)]` instead of `arr[seq]`. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   4169\u001B[0m            \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSee https://github.com/google/jax/issues/4564 for more information.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 4170\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg)\n\u001B[1;32m   4171\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   4172\u001B[0m   idx \u001B[38;5;241m=\u001B[39m (idx,)\n",
      "\u001B[0;31mTypeError\u001B[0m: Using a non-tuple sequence for multidimensional indexing is not allowed; use `arr[tuple(seq)]` instead of `arr[seq]`. See https://github.com/google/jax/issues/4564 for more information."
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ff82a65846a0d11c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
