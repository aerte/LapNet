{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import numpy as np\n",
    "\n",
    "# Define the custom transformation\n",
    "def scale_by_adaptive_magnitude(decay_rate=0.9, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Scales updates by the inverse of the running average of the gradient magnitudes.\n",
    "    \n",
    "    Args:\n",
    "    - decay_rate: Float, controls the smoothing of the running average.\n",
    "    - eps: Float, a small epsilon to prevent division by zero.\n",
    "    \n",
    "    Returns:\n",
    "    - An Optax transformation that scales updates adaptively.\n",
    "    \"\"\"\n",
    "    def init_fn(params):\n",
    "        # Initialize a state to hold the running averages of the gradient magnitudes\n",
    "        avg_grads = jax.tree_map(lambda p: jnp.zeros_like(p), params)\n",
    "        return avg_grads\n",
    "\n",
    "    def update_fn(updates, state, params=None):\n",
    "        # Compute the new average gradient magnitudes\n",
    "        new_avg_grads = jax.tree_multimap(\n",
    "            lambda g, avg_g: decay_rate * avg_g + (1 - decay_rate) * jnp.abs(g),\n",
    "            updates,\n",
    "            state\n",
    "        )\n",
    "        \n",
    "        # Scale updates by the inverse of the running average\n",
    "        scaled_updates = jax.tree_multimap(\n",
    "            lambda g, avg_g: g / (avg_g + eps),\n",
    "            updates,\n",
    "            new_avg_grads\n",
    "        )\n",
    "        \n",
    "        return scaled_updates, new_avg_grads\n",
    "\n",
    "    return optax.GradientTransformation(init_fn, update_fn)\n",
    "\n",
    "# Define the full custom optimizer\n",
    "def custom_optimizer():\n",
    "    return optax.chain(\n",
    "        scale_by_adaptive_magnitude(),                     # Custom transformation\n",
    "        optax.scale_by_schedule(optax.constant_schedule(0.1)),  # Constant learning rate schedule\n",
    "        optax.scale(-1.0)                                  # Scale to perform gradient descent\n",
    "    )\n",
    "\n",
    "# Initialize the optimizer with example parameters\n",
    "params = {'w': jnp.array([1.0, 2.0, 3.0]), 'b': jnp.array([0.5])}  # Example parameters\n",
    "optimizer = custom_optimizer()\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "# Define a dummy gradient for testing\n",
    "grads = {'w': jnp.array([0.1, -0.2, 0.15]), 'b': jnp.array([-0.05])}\n",
    "\n",
    "# Apply the custom optimizer's update\n",
    "updates, opt_state = optimizer.update(grads, opt_state)\n",
    "new_params = optax.apply_updates(params, updates)\n",
    "\n",
    "print(\"Updated parameters:\", new_params)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T21:36:15.338392Z",
     "start_time": "2024-11-09T21:36:14.147428Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import numpy as np\n",
    "\n",
    "BATCH_SIZE = 5\n",
    "NUM_TRAIN_STEPS = 1_000\n",
    "RAW_TRAINING_DATA = np.random.randint(255, size=(NUM_TRAIN_STEPS, BATCH_SIZE, 1))\n",
    "\n",
    "TRAINING_DATA = np.unpackbits(RAW_TRAINING_DATA.astype(np.uint8), axis=-1)\n",
    "LABELS = jax.nn.one_hot(RAW_TRAINING_DATA % 2, 2).astype(jnp.float32).reshape(NUM_TRAIN_STEPS, BATCH_SIZE, 2)\n",
    "\n",
    "initial_params = {\n",
    "    'hidden': jax.random.normal(shape=[8, 32], key=jax.random.PRNGKey(0)),\n",
    "    'output': jax.random.normal(shape=[32, 2], key=jax.random.PRNGKey(1)),\n",
    "}\n",
    "\n",
    "\n",
    "def net(x: jnp.ndarray, params: optax.Params) -> jnp.ndarray:\n",
    "  x = jnp.dot(x, params['hidden'])\n",
    "  x = jax.nn.relu(x)\n",
    "  x = jnp.dot(x, params['output'])\n",
    "  return x\n",
    "\n",
    "\n",
    "def loss(params: optax.Params, batch: jnp.ndarray, labels: jnp.ndarray) -> jnp.ndarray:\n",
    "  y_hat = net(batch, params)\n",
    "\n",
    "  # optax also provides a number of common loss functions.\n",
    "  loss_value = optax.sigmoid_binary_cross_entropy(y_hat, labels).sum(axis=-1)\n",
    "\n",
    "  return loss_value.mean()\n",
    "\n",
    "def fit(params: optax.Params, optimizer: optax.GradientTransformation) -> optax.Params:\n",
    "  opt_state = optimizer.init(params)\n",
    "\n",
    "  @jax.jit\n",
    "  def step(params, opt_state, batch, labels):\n",
    "    loss_value, grads = jax.value_and_grad(loss)(params, batch, labels)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss_value\n",
    "\n",
    "  for i, (batch, labels) in enumerate(zip(TRAINING_DATA, LABELS)):\n",
    "    params, opt_state, loss_value = step(params, opt_state, batch, labels)\n",
    "    if i % 100 == 0:\n",
    "      print(f'step {i}, loss: {loss_value}')\n",
    "\n",
    "  return params\n",
    "\n",
    "# Finally, we can fit our parametrized function using the Adam optimizer\n",
    "# provided by optax.\n",
    "optimizer = optax.adam(learning_rate=1e-2)\n",
    "params = fit(initial_params, optimizer)"
   ],
   "id": "57a5ca970d679fa5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: 14.04040813446045\n",
      "step 100, loss: 0.25464972853660583\n",
      "step 200, loss: 0.04770423099398613\n",
      "step 300, loss: 0.003774885553866625\n",
      "step 400, loss: 0.004785655532032251\n",
      "step 500, loss: 0.004779085982590914\n",
      "step 600, loss: 0.0013946484541520476\n",
      "step 700, loss: 0.001016934053041041\n",
      "step 800, loss: 0.03547176346182823\n",
      "step 900, loss: 0.007677190005779266\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T22:01:40.240647Z",
     "start_time": "2024-11-09T22:01:40.228766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import jax.random as jr\n",
    "import lineax as lx\n",
    "\n",
    "matrix_key, vector_key = jr.split(jr.PRNGKey(0))\n",
    "matrix = jr.normal(matrix_key, (10, 8))\n",
    "vector = jr.normal(vector_key, (10,))\n",
    "operator = lx.MatrixLinearOperator(matrix)\n",
    "solution = lx.linear_solve(operator, vector, solver=lx.QR())\n",
    "print(solution)"
   ],
   "id": "2a10db76e8df539d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution(\n",
      "  value=f32[8],\n",
      "  result=EnumerationItem(\n",
      "    _value=i32[],\n",
      "    _enumeration=<class 'lineax._solution.RESULTS'>\n",
      "  ),\n",
      "  stats={},\n",
      "  state=(\n",
      "    (f32[10,8], f32[8,8]),\n",
      "    False,\n",
      "    Static(\n",
      "      _leaves=[\n",
      "        ShapeDtypeStruct(shape=(10,), dtype=float32),\n",
      "        ShapeDtypeStruct(shape=(8,), dtype=float32),\n",
      "        PyTreeDef((*, *))\n",
      "      ],\n",
      "      _treedef=PyTreeDef(([*, *], *))\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3e9fdcef582649b9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
